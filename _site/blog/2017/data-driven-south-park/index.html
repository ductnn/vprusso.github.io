<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="msvalidate.01" content="5E7827CBC3E6F12865C285B3020A9C95" />

  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2017/data-driven-south-park/">
  <link rel="alternate" type="application/rss+xml" title="Vincent Russo" href="http://localhost:4000/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Vincent Russo</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/blog/">Blog</a>
          
        
          
        
          
          <a class="page-link" href="/cv/">Resume/CV</a>
          
        
          
        
          
          <a class="page-link" href="/">Home</a>
          
        
          
        
          
          <a class="page-link" href="/music/">Music</a>
          
        
          
          <a class="page-link" href="/projects/">Projects</a>
          
        
          
          <a class="page-link" href="/publications/">Publications</a>
          
        
          
        
          
        
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

    <header class="post-header">
        <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59145213-1', 'auto');
  ga('send', 'pageview');

</script>
        <h1 class="post-title">Data Driven South Park</h1>
        <p class="post-meta">Mar 24, 2017</p>
    </header>

    <article class="post-content">
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- blog -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7213376997288299"
     data-ad-slot="4540332365"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
        <p>Aside from seasons 1 through 9 of <em>“The Simpsons”</em>, the only other show that comes close to my all-time favorite is <em>“South Park”</em>. I grew up watching Cartman, Stan, Kyle, and Kenny, and I’m sure many people reading this have as well.</p>

<p>I was particularly excited to see that one of the users on <a href="https://www.kaggle.com">Kaggle</a> (<a href="https://www.kaggle.com/tovarischsukhov">Ksenia Sukhova</a>) uploaded an <a href="https://www.kaggle.com/tovarischsukhov/southparklines"> entire line-by-line transcription of seasons 1 to 18 of <em>“South Park”</em></a>. What interesting things can we do with over 70,000 lines of dialog? Well, let’s find out!</p>

<p>This post is split up into two primary sections:</p>

<ul>
  <li>
    <p><a href="#basic-analysis-and-natural-language-processing">Analysis and Natural Language Processing</a>:
We do some topical data analysis on the dialog, complete with pretty graphs. If you wish to perform your own analysis, the tools to do so are provided in this section.</p>
  </li>
  <li>
    <p><a href="#recurrent-neural-networks-and-south-park">Recurrent Neural Networks and South Park</a>:
We construct a neural network that learns from the “<em>South Park</em>” dialog. If you wish to train your own neural network on a large corpus of your own text, we provide a guide to do so.</p>
  </li>
</ul>

<p>All of the software and supplementary files I wrote for this post is available on <a href="https://github.com/vprusso/south_park_data">my Github page</a>.</p>

<h2 id="basic-analysis-and-natural-language-processing">Basic Analysis and Natural Language Processing</h2>

<p>We start off by investigating some high-level details that may be extracted from the “<em>South Park</em>” dialog.</p>

<h4 id="swearing-frequency">Swearing Frequency</h4>

<p>It’s no secret that swearing is a common occurrence on “<em>South Park</em>”. For certain swear words, we can take a peek and see just how common they are for each of the 18 seasons. The following graph plots the frequency of words “Ass”, “Damn/Dammit”, “Fuck”, and “Shit” across each of the 18 seasons.</p>
<p align="center">
    <center>
        <figure>
            <img src="http://i.imgur.com/mdYP6A7.png" alt="Plot for occurence of swear word by South Park season." />
        </figure>
    </center>
</p>
<p>The word “ass” started off really strong in the earlier seasons has progressively declined. We see a spike in season 5 for the word “shit” that is attributed to one particular episode.</p>

<p>Episode 1 of season 5 entitled <em>“It Hits the Fan”</em> was noteworthy mostly for the abundant use of the word “shit” throughout the entire episode. An excerpt from <a href="http://www.nytimes.com/2001/06/25/business/mediatalk-south-park-takes-gross-to-new-frontier.html">an article by the New York Times</a> states that</p>

<blockquote>
  <p>Throughout the episode, the profanity “shit” or “shitty” are exclaimed uncensored a total of 162 separate times; in syndicated or re-aired versions of this episode, a counter in the bottom left corner of the screen counts the number of times the word has been uttered. The written occurrences are not counted, but “shit” is written 38 times, which puts the “shit”s up to an even 200. “Shit” is uttered roughly once every eight seconds; one such count includes the episode’s theme song in the calculation.</p>
</blockquote>

<h4 id="the-loudest-mouth">The Loudest Mouth</h4>

<p>Out of the primary four <em>South Park</em> characters, Cartman, Stan, Kyle, and Kenny, which one has the loudest mouth? We can start off by analyzing how many lines of dialog each character has in the entire show.</p>

<p align="center">
    <center>
        <figure>
            <img src="http://i.imgur.com/u4nIjQq.png" alt="Plot for total lines of dialog per character on South Park." />
        </figure>
    </center>
</p>

<p>For anyone who has watched the show, this comes as no surprise. Cartman is undoubtedly the one with the biggest mouth, and Kenny is hardly audible through his orange parka.</p>

<h4 id="the-dirtiest-mouth">The Dirtiest Mouth</h4>

<p>We can also ask a similar question about which one of Cartman, Stan, Kyle, and Kenny has the dirtiest mouth, or uses the most swear words. Starting from the top-left, the above pie charts correspond clockwise to “Ass”, “Damn/Dammit”, “Fuck”, and “Shit”, respectively in the following graphs.</p>

<p align="center">
    <center>
        <figure>
            <img src="http://i.imgur.com/Fam7wU4.png" alt="Proportion of swear words spoken per character on South Park." />
        </figure>
    </center>
</p>

<p>Again, not much of a surprise here, as Cartman appears to be responsible for the majority of the swearing between the boys.</p>

<h4 id="most-common-words">Most Common Words</h4>

<p>The following plot gives the 50 most common words used in the dialog for the entire series of “<em>South Park</em>”. I decided to not include “stop words” (words that are common and relatively meaningless from an analysis perspective: “and”, “it”, “I”, etc.). I also eliminated a number of what I considered to be “filler words” (words such as: “oh”, “get”, and “could”, etc.). The resulting plot gives the top 50 words used in the entire dialog.</p>

<p align="center">
    <center>
        <figure>
            <img src="http://i.imgur.com/XJ8uT8v.png" alt="Most common words in South Park." />
        </figure>
    </center>
</p>

<h4 id="performing-your-own-analysis">Performing Your Own Analysis</h4>

<p>I’ve only just scratched the surface in deriving insights from the “<em>South Park</em>” data. If you’re so inclined, I encourage you to clone my <a href="https://github.com/vprusso/south_park_data">Github repository</a> and analyze the data yourself. You’ll require <a href="https://www.python.org/downloads/">Python</a> as well as the <a href="http://pandas.pydata.org/">pandas</a> and <a href="http://www.nltk.org/">nltk</a> toolkits installed. If you install Python’s pip:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo apt-get install python-pip
</code></pre>
</div>

<p>You’ll be able to install the other necessary packages using pip:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo pip install numpy
sudo pip install pandas
sudo pip install nltk
</code></pre>
</div>

<p>You’ll also require the code on my repository, that may be cloned as</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/vprusso/south_park_data.git
</code></pre>
</div>

<p>Once you’ve cloned the repository, you can load up the data into a pandas dataframe</p>

<div class="highlighter-rouge"><pre class="highlight"><code>df = read_south_park_data('sp_season_dialog.csv')
</code></pre>
</div>

<p>Here are a few examples of questions you can ask using the “<em>South Park</em>” data as well as my code:</p>

<p><em>Question:</em> How many times is the word “dude” said in all of the seasons 1 to 18:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>print(word_count_by_season_and_episode(df, word="dude"))
2087
</code></pre>
</div>

<p><em>Question:</em> How many times is the word “dude” said in all of season 1:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>print(word_count_by_season_and_episode(df, word="dude", seasons=['1']))
139
</code></pre>
</div>

<p><em>Question:</em> How many times is the word “dude” said in seasons 1,2,3, and 10:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>print(word_count_by_season_and_episode(df, word="dude", seasons=['1','2','3','10']))
644
</code></pre>
</div>

<p><em>Question:</em> How many times is the word “dude” said in season 1 and in episodes 1 and 2:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>print(word_count_by_season_and_episode(df, word="dude", seasons=['1'], episodes=['1','2']))
21
</code></pre>
</div>

<p><em>Question:</em> How many times is the word “dude” said in all of the seasons 1 to 18 by Cartman:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>print(word_count_by_season_and_episode(df, word="dude", character="Cartman"))
427
</code></pre>
</div>

<p>This is just a small sample of the flavor of questions one may ask. If you manage to do something neat with this data, please do post a link in the comments!</p>

<h2 id="recurrent-neural-networks-and-south-park">Recurrent Neural Networks and South Park</h2>

<h3 id="a-south-park-based-recurrent-neural-network">A South Park-based Recurrent Neural Network</h3>

<p>In this section, we’ll be training a recurrent neural network on all of the <em>“South Park”</em> dialog from seasons 1 to 18. The result will be a model that will ideally output syntactic structure similar to that of a character on <em>“South Park”</em>.</p>

<p>In order to achieve this, I followed Andrej’s excellent <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog post on recurrent neural networks</a>. I also used the software package Andrej wrote, <a href="https://github.com/karpathy/char-rnn">char-rnn</a>, as well as the slightly improved version by Justin Johnson under the name <a href="https://github.com/jcjohnson/torch-rnn">torch-rnn</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Neural networks</a> have seen a resurgence since their original inception since arguably the 1940s. When neural networks were originally devised, they served more as a theoretical construct than a practical one due to computational constraints. With our present levels of computing power, neural networks have become more practical and have been used in a number of very intriguing ways.</p>

<p>This post is surely not broad enough to cover the expansive landscape of neural networks. However, there are a number of wonderful resources on the topic including Michael Nielsen’s book entitled <a href="http://neuralnetworksanddeeplearning.com/">“Neural Networks and Deep Learning”</a>. If you’d like to see some other practical examples of neural networks, the <a href="https://www.tensorflow.org/get_started/get_started">TensorFlow tutorial</a> is also very well written. I also have another post where I used a <a href="http://vprusso.github.io/blog/2016/tensor-flow-neural-net-breast-cancer/">neural network created by TensorFlow to distinguish between either malignant or benign breast cancer tumors</a> that may also be of interest.</p>

<p>To train the recurrent neural network, I extracted lines from the <em>“South Park”</em> dialog data and fed this in as input. I used the <a href="https://github.com/jcjohnson/torch-rnn">torch-rnn</a> software to perform the training (more detail on this is in the next section). After the neural network was trained I invoked the torch-rnn interface to determine what syntactic structure the model could produce.</p>

<p>Keep in mind that while the sentences may sound a bit nonsensical the neural network started with nothing. That is, it learned grammatical structure, punctuation, and to a greater extent how English functions, etc. in addition to constructing sentences in a style similar to that of “<em>South Park</em>”. I should also mention that since I wasn’t training on a machine with a GPU, I had less time to play around with some of the settings of how the neural network learns to see if different parameters gave better results. Even so, here are a few interesting lines the neural network was able to generate:</p>

<blockquote>
  <p>Ah, where’s my ass! We’re all gonna screw me together.</p>
</blockquote>

<blockquote>
  <p>Cartman! A Magic Cartman.</p>
</blockquote>

<blockquote>
  <p>No way, “Who was crapping with the seat down?” Hella no!</p>
</blockquote>

<blockquote>
  <p>Yes, children, this is!</p>
</blockquote>

<p>I don’t think Matt Stone and Trey Parker will be writing in this character anytime soon, but who knows!</p>

<h3 id="train-your-own-recurrent-neural-network">Train Your Own Recurrent Neural Network</h3>

<p>Maybe you’d like to train your own neural network on a corpus of text that you have. As <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej’s blog post</a> mentions, these recurrent neural networks are very effective at determining structure in not only basic text, but also software syntax, mathematical equations, etc.</p>

<p>In this section, I’ll tell you precisely how to train your recurrent neural network, from start to finish. I’ll go into more detail about how to get each of these working, but the general overview will be based on:</p>

<ul>
  <li><a href="https://m.do.co/c/8e3f1a477d6d">DigitalOcean</a>,</li>
  <li><a href="http://torch.ch/docs/getting-started.html#_">Torch</a>,</li>
  <li><a href="">torch-rnn</a> (a slightly faster version of char-rnn).</li>
</ul>

<p>For this example, we’ll be using the <a href="">dialog text on my Github page</a> as our training data. You’re free to use any text file you’d like. If you want to use your own data, that’s great, but keep in mind that generally the more data you have to work with, the better.</p>

<h4 id="1-create-a-droplet-at-digitalocean">1. Create a droplet at <a href="https://m.do.co/c/8e3f1a477d6d">DigitalOcean</a>.</h4>

<p>In order for this tutorial to be as general as possible, I’m assuming you either have a personal machine with Ubuntu on it or if you don’t, you can rent computing time at Digital Ocean. I used this service myself for this blog post and cannot recommend their services highly enough.</p>

<p>The wonderful thing about <a href="https://m.do.co/c/8e3f1a477d6d">DigitalOcean</a> is you pay for only the computing time that you use. If you want to spin up an Ubuntu instance and only use it for the time necessary to train the network and then destroy the instance, you are only charged for the time and computing power you use.</p>

<p>Again, if you have your own Ubuntu machine, feel free to follow along, or spin up one at <a href="https://m.do.co/c/8e3f1a477d6d">DigitalOcean</a> by following the steps laid out in the <a href="https://www.digitalocean.com/community/tutorials/how-to-create-your-first-digitalocean-droplet-virtual-server">“How to Create Your First DigitalOcean Droplet Virtual Server”</a>.</p>

<h4 id="2-install-dependencies">2. Install dependencies.</h4>

<p>This step will closely follow the documentation for the <a href="">torch-rnn project</a>. Once you have your droplet setup, SSH into it (<a href="https://www.digitalocean.com/community/tutorials/how-to-create-your-first-digitalocean-droplet-virtual-server#step-nine-—-log-in-to-your-droplet">Step 9 in the Digital Ocean tutorial</a>). Once you’re logged in, run the following commands to install Python and a library for processing HDF5 files:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo apt-get -y install python2.7-dev
sudo apt-get install libhdf5-dev
</code></pre>
</div>

<p>The neural network requires the <a href="">Torch</a> framework. To install Torch, run the following commands:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/torch/distro.git ~/torch --recursive
cd ~/torch; bash install-deps;
./install.sh
source ~/.bashrc
</code></pre>
</div>

<p>This will take some time to install, as the framework is quite large.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>luarocks install torch
luarocks install nn
luarocks install optim
luarocks install lua-cjson
</code></pre>
</div>

<p>Now we can go ahead and clone the torch-hdf5 from Github.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd ..
git clone https://github.com/deepmind/torch-hdf5
cd torch-hdf5
luarocks make hdf5-0-0.rockspec
</code></pre>
</div>

<p>We’ve got the minimal amount of what we need now to train our network. You can get more fancy with it by enabling CUDA support for GPUs. I encourage you to do so if you’re so inclined, but it’s not necessary. Enabling the GPU will make training the neural network faster, but in this example, we won’t be using them.</p>

<h4 id="3-train-the-neural-network">3. Train the neural network.</h4>

<p>Upload your training data to the DigitalOcean droplet you’ve created (in our example, that’s the <a href=""><em>South Park</em>”” dialog data</a> entitled <code class="highlighter-rouge">dialog.txt</code>).</p>

<p>Navigate to the torch-rnn directory via the terminal.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd torch-rnn
</code></pre>
</div>

<p>Before we train the network, we have to convert our <code class="highlighter-rouge">dialog.txt</code> file to an HDF5 and JSON format that torch-rnn can understand and process. Assuming your <code class="highlighter-rouge">dialog.txt</code> is in the torch-rnn directory, run the following command.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>python scripts/preprocess.py \
    --input_txt dialog.txt \
    --output_h5 dialog.h5 \
    --output_json dialog.json
</code></pre>
</div>

<p>Alright, our training data is ready to be fed into the neural network. Before we train the network, let’s create a Linux screen so that we can start our process, logout if we wish, and log back in to check on how it’s going.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>screen -S neural-net-dialog
</code></pre>
</div>

<p>This will open a new screen where we will start to train the network.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>th train.lua -input_h5 dialog.h5 -input_json dialog.json -gpu -1
</code></pre>
</div>

<p>Once this starts running, we can detach from our screen by pressing CTRL-A CTRL-D. You can then logout if you wish. If you want to take a peek at how the processing is going, SSH back into the DigitalOcean droplet and run</p>

<div class="highlighter-rouge"><pre class="highlight"><code>screen -ls
</code></pre>
</div>

<p>to get a list of all of the screens running. If you want to resume the screen, just type</p>

<div class="highlighter-rouge"><pre class="highlight"><code>screen -r N
</code></pre>
</div>

<p>where N is the number proceeding the name that you gave the screen. In our case, this was “neural-net-dialog”. You can kill any screens that have finished by running</p>

<div class="highlighter-rouge"><pre class="highlight"><code>screen -X -S N
</code></pre>
</div>

<p>where again, N is the number proceeding the name of the screen. If you’re not familiar with Linux screens, you can read more about them at the <a href="https://ss64.com/bash/screen.html">screen MAN page</a>.</p>

<h4 id="4-analyze-the-output">4. Analyze the output.</h4>

<p>Depending on how large the training dataset is, training the neural network will take a while. In the case of the <code class="highlighter-rouge">dialog.txt</code> data, it takes about 24 hours to complete training.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>th sample.lua -checkpoint cv/checkpoint_10000.t7 -length 2000 -gpu -1
</code></pre>
</div>

<p>where checkpoint_X is the name of the checkpoint file generated in the <code class="highlighter-rouge">cv</code> folder. This command will sample 2000 tokens of text from the trained recurrent neural network model.</p>

<p>And that’s pretty much it. If you have any questions or comments on this, please feel free to comment on this post and I’ll do my best to respond as soon as I can. Please be sure to thank both <a href="http://karpathy.github.io/">Andrej Karpath</a> and <a href="http://cs.stanford.edu/people/jcjohns/">Justin Johnson</a> for their awesome neural network tools as well as <a href="https://www.kaggle.com/tovarischsukhov">Ksenia Sukhova</a> for extracting the “<em>South Park</em>” data and to <a href="https://www.kaggle.com/">Kaggle</a> for hosting it, and let me know in the comments if you manage to do something neat from this tutorial!</p>

        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- blog -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-7213376997288299"
     data-ad-slot="4540332365"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
    </article>
  
    
        <div id="disqus_thread"></div>
            <script type="text/javascript">
                /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
                var disqus_shortname = 'vprusso'; // required: replace example with your forum shortname
                // var disqus_developer = 1; // Comment out when the site is live
                var disqus_identifier = "/blog/2017/data-driven-south-park/";

                /* * * DON'T EDIT BELOW THIS LINE * * */
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    

    
    <h2>Related Posts:</h2>
    <ul>
      
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
          
        
          
        
      
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      

      
      
        
          

          
          
            
          
            
          
            
          
          
            
            <li>
              <h3>
                <a href="/blog/2018/natural-language-processing-python-5/">
                  Natural Language Processing in Python: Part 5 -- Stemming and Lemmatization
                  <small>10 Jan 2018</small>
                </a>
              </h3>
            </li>
          
        
          

          
          
            
          
            
          
            
          
          
            
            <li>
              <h3>
                <a href="/blog/2018/natural-language-processing-python-4/">
                  Natural Language Processing in Python: Part 4 -- WordNet
                  <small>09 Jan 2018</small>
                </a>
              </h3>
            </li>
          
        
          

          
          
            
          
            
          
            
          
          
            
            <li>
              <h3>
                <a href="/blog/2018/natural-language-processing-python-3/">
                  Natural Language Processing in Python: Part 3 -- Generating Word Clouds
                  <small>08 Jan 2018</small>
                </a>
              </h3>
            </li>
          
        
          
            
      
    </ul>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Vincent Russo</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Vincent Russo</li>
          <li><a href="mailto:vincentrusso1@gmail.com">vincentrusso1@gmail.com</a></li>
        </ul>
      </div>

	  <div class="footer-col  footer-col-2">
	   <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/vprusso">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">Github</span>
            </a>
          </li>
          
		  
	  
		  <li>
		    <a href="https://ca.linkedin.com/in/vrusso11" class="icon-17 linkedin" title="LinkedIn">
		      <span class="icon  icon--linkedin">
		       <svg viewBox="0 0 512 512">
			<path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z"
			/>
		      </svg>
		      </span>
		      <span class="username">Linkedin</span>
		      <!--[if lt IE 9]><em>LinkedIn</em><![endif]--></a>
		  </li>
	  
	  
	  
		<li>
			<a href="https://scholar.google.ca/citations?user=dJEXPiQAAAAJ&hl=en" class="icon  icon--googlescholar" title="GoogleScholar">
			<span class="icon icon--googlescholar">
			<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" viewBox="0 0 16 16">
			<path fill="#000000" d="M14.942 12.57l-4.942-8.235v-3.335h0.5c0.275 0 0.5-0.225 0.5-0.5s-0.225-0.5-0.5-0.5h-5c-0.275 0-0.5 0.225-0.5 0.5s0.225 0.5 0.5 0.5h0.5v3.335l-4.942 8.235c-1.132 1.886-0.258 3.43 1.942 3.43h10c2.2 0 3.074-1.543 1.942-3.43zM3.766 10l3.234-5.39v-3.61h2v3.61l3.234 5.39h-8.468z"></path>
			</svg>
			</span>
			<span class="username">Google Scholar</span>
			<!--[if lt IE 9]><em>GoogleScholar</em><![endif]--></a>
		</li>
	  
	  
        </ul>
      </div>
	  
      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
      
	  	  
			<li><a href="http://stackexchange.com/users/412832/vincent-russo" class="icon-23 stackoverflow" title="StackOverflow">
			<span class="icon  icon--stackoverflow">
			<svg viewBox="0 0 512 512">
			<path d="M294.8 361.2l-122 0.1 0-26 122-0.1L294.8 361.2zM377.2 213.7L356.4 93.5l-25.7 4.5 20.9 120.2L377.2 213.7zM297.8 301.8l-121.4-11.2 -2.4 25.9 121.4 11.2L297.8 301.8zM305.8 267.8l-117.8-31.7 -6.8 25.2 117.8 31.7L305.8 267.8zM321.2 238l-105-62 -13.2 22.4 105 62L321.2 238zM346.9 219.7l-68.7-100.8 -21.5 14.7 68.7 100.8L346.9 219.7zM315.5 275.5v106.5H155.6V275.5h-20.8v126.9h201.5V275.5H315.5z"/>
			</svg>
			</span>
			<span class="username">StackOverflow</span>
			<!--[if lt IE 9]><em>StackOverflow</em><![endif]--></a></li>
		 

          
          <li>
            <a href="https://twitter.com/captainhamptons">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">Twitter</span>
            </a>
          </li>
          
		  
		  
          <li>
            <a href="https://soundcloud.com/captainhampton">
              <span class="icon  icon--soundcloud">
				<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" viewBox="0 0 16 16">
					<path fill="#000000" d="M13.937 8.034c-0.283 0-0.552 0.055-0.798 0.154-0.164-1.787-1.723-3.188-3.625-3.188-0.465 0-0.917 0.088-1.317 0.237-0.156 0.058-0.197 0.117-0.197 0.233v6.292c0 0.121 0.098 0.222 0.221 0.234 0.005 0.001 5.68 0.003 5.717 0.003 1.139 0 2.062-0.888 2.062-1.983s-0.924-1.983-2.063-1.983zM6.25 12h0.5l0.25-3.503-0.25-3.497h-0.5l-0.25 3.497zM4.75 12h-0.5l-0.25-2.543 0.25-2.457h0.5l0.25 2.5zM2.25 12h0.5l0.25-2-0.25-2h-0.5l-0.25 2zM0.25 11h0.5l0.25-1-0.25-1h-0.5l-0.25 1z"></path>
				</svg>
              </span>

              <span class="username">Soundcloud</span>
            </a>
          </li>
          
		  
		  
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text"></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
